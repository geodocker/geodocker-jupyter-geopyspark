{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Dependencies](https://spacenetchallenge.github.io/#Dependencies)\n",
    "> The [AWS Command Line Interface (CLI)](https://aws.amazon.com/cli/) must be installed with an active AWS account. Configure the AWS CLI using ‘aws configure’"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Accessing the SpaceNet Data on AWS](https://aws.amazon.com/public-datasets/spacenet/#Accessing_the_SpaceNet_Data_on_AWS)\n",
    "> The SpaceNet dataset is being released in several Areas of Interest. All AOIs will follow a similar directory structure and data format. The imagery is GeoTIFF satellite imagery and corresponding GeoJSON building footprints. You can use the following [aws-cli](https://aws.amazon.com/cli/) command to examine all files available in the dataset (details of file structure below):\n",
    "\n",
    "> `aws s3 ls spacenet-dataset --request-payer requester`\n",
    "\n",
    "> For more detailed information on how to access specific files within the dataset, see [here](https://github.com/SpaceNetChallenge/utilities/tree/master/content/download_instructions).\n",
    "\n",
    "> _The spacenet-dataset S3 bucket is provided as a Requester Pays bucket, see [here](https://docs.aws.amazon.com/AmazonS3/latest/dev/RequesterPaysBuckets.html) for more information._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading Rio raster and vector data with [Boto](https://boto3.readthedocs.io/en/latest/index.html)\n",
    "Since the bucket is Request Pays, we cannot successfully curl images. Instead, Boto, the AWS SDK for Python, provides an interface to download files from Request Pays buckets. The [S3Transfer](https://boto3.readthedocs.io/en/latest/reference/customizations/s3.html#boto3.s3.transfer.S3Transfer) class has a download method that can take in a 'RequestPayer' argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "\n",
    "bucket = \"spacenet-dataset\"\n",
    "\n",
    "aoi_path = \"AOI_1_Rio\"\n",
    "aoi_data_path = os.path.join(aoi_path, \"srcData\")\n",
    "building_labels_path = os.path.join(aoi_data_path, \"buildingLabels\")\n",
    "mosaic_3band_path = os.path.join(aoi_data_path, \"mosaic_3band\")\n",
    "\n",
    "client = boto3.client(\"s3\")\n",
    "transfer = boto3.s3.transfer.S3Transfer(client)\n",
    "\n",
    "def download_if_not_exists(key, filename):\n",
    "    if not os.path.exists(filename):\n",
    "        transfer.download_file(\n",
    "            bucket=bucket, key=key, filename=filename,\n",
    "            extra_args={\"RequestPayer\": \"requester\"})\n",
    "\n",
    "mosaic_3band_object_list = client.list_objects_v2(\n",
    "    Bucket=bucket, Prefix=mosaic_3band_path,\n",
    "    RequestPayer='requester')\n",
    "mosaic_3band_key = [obj[\"Key\"] for obj in mosaic_3band_object_list[\"Contents\"]][0]\n",
    "mosaic_3band_filename = os.path.join(\"/tmp\", mosaic_3band_key.split(\"/\")[-1])\n",
    "download_if_not_exists(mosaic_3band_key, mosaic_3band_filename)\n",
    "\n",
    "outline_filename = \"Rio_OUTLINE_Public_AOI.geojson\"\n",
    "outline_key = os.path.join(building_labels_path, outline_filename)\n",
    "download_if_not_exists(outline_key, outline_filename)\n",
    "\n",
    "buildings_filename = \"Rio_Buildings_Public_AOI_v2.geojson\"\n",
    "buildings_key = os.path.join(building_labels_path, buildings_filename)\n",
    "download_if_not_exists(buildings_key, buildings_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangling imagery with [GDAL](http://www.gdal.org/gdal_translate.html)\n",
    "Since \"Compression type JPEG is not supported by [this reader](https://github.com/locationtech/geotrellis/blob/master/raster/src/main/scala/geotrellis/raster/io/geotiff/compression/Decompressor.scala#L119-L122)\" at the time of this demo, we need to [gdal_translate](http://www.gdal.org/gdal_translate.html) the image with a different compression type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal\n",
    "\n",
    "catalog_uri = os.path.join(\"/tmp\", \"catalog.tif\")\n",
    "\n",
    "if not os.path.exists(catalog_uri):\n",
    "    gdal.Translate(\n",
    "        destName=catalog_uri, srcDS=mosaic_3band_filename,\n",
    "        creationOptions=['COMPRESS=LZW']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingesting imagery for fast viewing with [GeoPySpark](https://github.com/locationtech-labs/geopyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o29.reproject.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 1.0 failed 1 times, most recent failure: Lost task 1.0 in stage 1.0 (TID 2, localhost, executor driver): java.io.FileNotFoundException: File file:/home/hadoop/notebooks/catalog.tif does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)\n\tat geotrellis.spark.io.hadoop.HdfsUtils$.readRange(HdfsUtils.scala:185)\n\tat geotrellis.spark.io.hadoop.HdfsRangeReader.readClippedRange(HdfsRangeReader.scala:39)\n\tat geotrellis.util.RangeReader$class.readRange(RangeReader.scala:36)\n\tat geotrellis.spark.io.hadoop.HdfsRangeReader.readRange(HdfsRangeReader.scala:31)\n\tat geotrellis.util.StreamingByteReader$$anonfun$1.apply(StreamingByteReader.scala:90)\n\tat geotrellis.util.StreamingByteReader$$anonfun$1.apply(StreamingByteReader.scala:90)\n\tat geotrellis.util.StreamingByteReader$Chunk.data(StreamingByteReader.scala:43)\n\tat geotrellis.util.StreamingByteReader$Chunk.buffer$lzycompute(StreamingByteReader.scala:48)\n\tat geotrellis.util.StreamingByteReader$Chunk.buffer(StreamingByteReader.scala:48)\n\tat geotrellis.util.StreamingByteReader.getChar(StreamingByteReader.scala:110)\n\tat geotrellis.raster.io.geotiff.reader.TiffTagsReader$.read(TiffTagsReader.scala:83)\n\tat geotrellis.raster.io.geotiff.reader.GeoTiffReader$.readGeoTiffInfo(GeoTiffReader.scala:289)\n\tat geotrellis.raster.io.geotiff.reader.GeoTiffReader$.readMultiband(GeoTiffReader.scala:175)\n\tat geotrellis.raster.io.geotiff.MultibandGeoTiff$.streaming(MultibandGeoTiff.scala:120)\n\tat geotrellis.spark.io.RasterReader$$anon$2.readWindow(RasterReader.scala:115)\n\tat geotrellis.spark.io.RasterReader$$anon$2.readWindow(RasterReader.scala:107)\n\tat geotrellis.spark.io.hadoop.HadoopGeoTiffRDD$$anonfun$apply$5.apply(HadoopGeoTiffRDD.scala:161)\n\tat geotrellis.spark.io.hadoop.HadoopGeoTiffRDD$$anonfun$apply$5.apply(HadoopGeoTiffRDD.scala:153)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)\n\tat scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1336)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1011)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1009)\n\tat org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1980)\n\tat org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1980)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1981)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1025)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1007)\n\tat geopyspark.geotrellis.RasterSummary$.collect(RasterSummary.scala:61)\n\tat geopyspark.geotrellis.ProjectedRasterLayer.tileToLayout(ProjectedRasterLayer.scala:66)\n\tat geopyspark.geotrellis.ProjectedRasterLayer.reproject(ProjectedRasterLayer.scala:81)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.FileNotFoundException: File file:/home/hadoop/notebooks/catalog.tif does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)\n\tat geotrellis.spark.io.hadoop.HdfsUtils$.readRange(HdfsUtils.scala:185)\n\tat geotrellis.spark.io.hadoop.HdfsRangeReader.readClippedRange(HdfsRangeReader.scala:39)\n\tat geotrellis.util.RangeReader$class.readRange(RangeReader.scala:36)\n\tat geotrellis.spark.io.hadoop.HdfsRangeReader.readRange(HdfsRangeReader.scala:31)\n\tat geotrellis.util.StreamingByteReader$$anonfun$1.apply(StreamingByteReader.scala:90)\n\tat geotrellis.util.StreamingByteReader$$anonfun$1.apply(StreamingByteReader.scala:90)\n\tat geotrellis.util.StreamingByteReader$Chunk.data(StreamingByteReader.scala:43)\n\tat geotrellis.util.StreamingByteReader$Chunk.buffer$lzycompute(StreamingByteReader.scala:48)\n\tat geotrellis.util.StreamingByteReader$Chunk.buffer(StreamingByteReader.scala:48)\n\tat geotrellis.util.StreamingByteReader.getChar(StreamingByteReader.scala:110)\n\tat geotrellis.raster.io.geotiff.reader.TiffTagsReader$.read(TiffTagsReader.scala:83)\n\tat geotrellis.raster.io.geotiff.reader.GeoTiffReader$.readGeoTiffInfo(GeoTiffReader.scala:289)\n\tat geotrellis.raster.io.geotiff.reader.GeoTiffReader$.readMultiband(GeoTiffReader.scala:175)\n\tat geotrellis.raster.io.geotiff.MultibandGeoTiff$.streaming(MultibandGeoTiff.scala:120)\n\tat geotrellis.spark.io.RasterReader$$anon$2.readWindow(RasterReader.scala:115)\n\tat geotrellis.spark.io.RasterReader$$anon$2.readWindow(RasterReader.scala:107)\n\tat geotrellis.spark.io.hadoop.HadoopGeoTiffRDD$$anonfun$apply$5.apply(HadoopGeoTiffRDD.scala:161)\n\tat geotrellis.spark.io.hadoop.HadoopGeoTiffRDD$$anonfun$apply$5.apply(HadoopGeoTiffRDD.scala:153)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)\n\tat scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1336)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1011)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1009)\n\tat org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1980)\n\tat org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1980)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2e3ac5672952>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     num_partitions=500)\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mlaid_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile_to_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGlobalLayout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_crs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3857\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mreprojected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlaid_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreproject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"EPSG:3857\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mpyramided\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreprojected\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyramid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_zoom\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_zoom\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hadoop/.local/lib/python3.4/site-packages/geopyspark/geotrellis/layer.py\u001b[0m in \u001b[0;36mtile_to_layout\u001b[0;34m(self, layout, target_crs, resample_method)\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtarget_crs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m             \u001b[0mtarget_crs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrs_to_proj4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_crs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_reproject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_crs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hadoop/.local/lib/python3.4/site-packages/geopyspark/geotrellis/layer.py\u001b[0m in \u001b[0;36m_reproject\u001b[0;34m(target_crs, layout, resample_method, layer)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mLocalLayout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGlobalLayout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLayoutDefinition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0msrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreproject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_crs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mTiledRasterLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrdd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o29.reproject.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 1.0 failed 1 times, most recent failure: Lost task 1.0 in stage 1.0 (TID 2, localhost, executor driver): java.io.FileNotFoundException: File file:/home/hadoop/notebooks/catalog.tif does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)\n\tat geotrellis.spark.io.hadoop.HdfsUtils$.readRange(HdfsUtils.scala:185)\n\tat geotrellis.spark.io.hadoop.HdfsRangeReader.readClippedRange(HdfsRangeReader.scala:39)\n\tat geotrellis.util.RangeReader$class.readRange(RangeReader.scala:36)\n\tat geotrellis.spark.io.hadoop.HdfsRangeReader.readRange(HdfsRangeReader.scala:31)\n\tat geotrellis.util.StreamingByteReader$$anonfun$1.apply(StreamingByteReader.scala:90)\n\tat geotrellis.util.StreamingByteReader$$anonfun$1.apply(StreamingByteReader.scala:90)\n\tat geotrellis.util.StreamingByteReader$Chunk.data(StreamingByteReader.scala:43)\n\tat geotrellis.util.StreamingByteReader$Chunk.buffer$lzycompute(StreamingByteReader.scala:48)\n\tat geotrellis.util.StreamingByteReader$Chunk.buffer(StreamingByteReader.scala:48)\n\tat geotrellis.util.StreamingByteReader.getChar(StreamingByteReader.scala:110)\n\tat geotrellis.raster.io.geotiff.reader.TiffTagsReader$.read(TiffTagsReader.scala:83)\n\tat geotrellis.raster.io.geotiff.reader.GeoTiffReader$.readGeoTiffInfo(GeoTiffReader.scala:289)\n\tat geotrellis.raster.io.geotiff.reader.GeoTiffReader$.readMultiband(GeoTiffReader.scala:175)\n\tat geotrellis.raster.io.geotiff.MultibandGeoTiff$.streaming(MultibandGeoTiff.scala:120)\n\tat geotrellis.spark.io.RasterReader$$anon$2.readWindow(RasterReader.scala:115)\n\tat geotrellis.spark.io.RasterReader$$anon$2.readWindow(RasterReader.scala:107)\n\tat geotrellis.spark.io.hadoop.HadoopGeoTiffRDD$$anonfun$apply$5.apply(HadoopGeoTiffRDD.scala:161)\n\tat geotrellis.spark.io.hadoop.HadoopGeoTiffRDD$$anonfun$apply$5.apply(HadoopGeoTiffRDD.scala:153)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)\n\tat scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1336)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1011)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1009)\n\tat org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1980)\n\tat org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1980)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1981)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1025)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1007)\n\tat geopyspark.geotrellis.RasterSummary$.collect(RasterSummary.scala:61)\n\tat geopyspark.geotrellis.ProjectedRasterLayer.tileToLayout(ProjectedRasterLayer.scala:66)\n\tat geopyspark.geotrellis.ProjectedRasterLayer.reproject(ProjectedRasterLayer.scala:81)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.FileNotFoundException: File file:/home/hadoop/notebooks/catalog.tif does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)\n\tat geotrellis.spark.io.hadoop.HdfsUtils$.readRange(HdfsUtils.scala:185)\n\tat geotrellis.spark.io.hadoop.HdfsRangeReader.readClippedRange(HdfsRangeReader.scala:39)\n\tat geotrellis.util.RangeReader$class.readRange(RangeReader.scala:36)\n\tat geotrellis.spark.io.hadoop.HdfsRangeReader.readRange(HdfsRangeReader.scala:31)\n\tat geotrellis.util.StreamingByteReader$$anonfun$1.apply(StreamingByteReader.scala:90)\n\tat geotrellis.util.StreamingByteReader$$anonfun$1.apply(StreamingByteReader.scala:90)\n\tat geotrellis.util.StreamingByteReader$Chunk.data(StreamingByteReader.scala:43)\n\tat geotrellis.util.StreamingByteReader$Chunk.buffer$lzycompute(StreamingByteReader.scala:48)\n\tat geotrellis.util.StreamingByteReader$Chunk.buffer(StreamingByteReader.scala:48)\n\tat geotrellis.util.StreamingByteReader.getChar(StreamingByteReader.scala:110)\n\tat geotrellis.raster.io.geotiff.reader.TiffTagsReader$.read(TiffTagsReader.scala:83)\n\tat geotrellis.raster.io.geotiff.reader.GeoTiffReader$.readGeoTiffInfo(GeoTiffReader.scala:289)\n\tat geotrellis.raster.io.geotiff.reader.GeoTiffReader$.readMultiband(GeoTiffReader.scala:175)\n\tat geotrellis.raster.io.geotiff.MultibandGeoTiff$.streaming(MultibandGeoTiff.scala:120)\n\tat geotrellis.spark.io.RasterReader$$anon$2.readWindow(RasterReader.scala:115)\n\tat geotrellis.spark.io.RasterReader$$anon$2.readWindow(RasterReader.scala:107)\n\tat geotrellis.spark.io.hadoop.HadoopGeoTiffRDD$$anonfun$apply$5.apply(HadoopGeoTiffRDD.scala:161)\n\tat geotrellis.spark.io.hadoop.HadoopGeoTiffRDD$$anonfun$apply$5.apply(HadoopGeoTiffRDD.scala:153)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)\n\tat scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1336)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1011)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1009)\n\tat org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1980)\n\tat org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1980)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "import geopyspark as gps\n",
    "from pyspark import SparkContext\n",
    "conf = gps.geopyspark_conf(\"local[*]\", \"spacenet-ingest\")\n",
    "conf.set(key='spark.ui.enabled', value='true')\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "\n",
    "catalog_uri = \"file:///home/hadoop/notebooks/catalog.tif\"\n",
    "# The following operation takes about X seconds on a reasonably capable 4-core laptop\n",
    "rdd = gps.geotrellis.geotiff.get(\n",
    "    gps.geotrellis.constants.LayerType.SPATIAL, \n",
    "    catalog_uri,\n",
    "    max_tile_size=512,\n",
    "    num_partitions=500)\n",
    "\n",
    "laid_out = rdd.tile_to_layout(layout = gps.GlobalLayout(), target_crs=3857)\n",
    "reprojected = laid_out.reproject(\"EPSG:3857\").cache().repartition(600)\n",
    "pyramided = reprojected.pyramid(start_zoom=12, end_zoom=1)\n",
    "\n",
    "for tiled in pyramided:\n",
    "    gps.geotrellis.catalog.write(\"file:///tmp/spacenet-catalog\", \"spacenet-ingest\", tiled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Showing Rio’s outline, imagery, and building footprints on a map with [GeoNotebook](https://github.com/OpenGeoscience/geonotebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from geonotebook.wrappers import VectorData\n",
    "outline_vector = VectorData(outline_filename)\n",
    "outline_polygons = [polygon for polygon in outline_vector.polygons]\n",
    "outline_polygon = outline_polygons[0]\n",
    "outline_centroid = outline_polygon.centroid\n",
    "x = outline_centroid.x\n",
    "y = outline_centroid.y\n",
    "z = 12\n",
    "M.set_center(x, y, z);\n",
    "M.add_layer(outline_vector, name=outline_key);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def render_image(tile):\n",
    "    cells = tile.cells\n",
    "    # Color correct - use magic numbers\n",
    "    magic_min, magic_max = 4000, 15176\n",
    "    norm_range = magic_max - magic_min\n",
    "    cells = cells.astype('int32')\n",
    "    # Clamp cells\n",
    "    cells[(cells != 0) & (cells < magic_min)] = magic_min\n",
    "    cells[(cells != 0) & (cells > magic_max)] = magic_max\n",
    "    colored = ((cells - magic_min) * 255) / norm_range\n",
    "    (r, g, b) = (colored[2], colored[1], colored[0])\n",
    "    alpha = np.full(r.shape, 255)\n",
    "    alpha[(cells[0] == tile.no_data_value) & \\\n",
    "          (cells[1] == tile.no_data_value) & \\\n",
    "          (cells[2] == tile.no_data_value)] = 0\n",
    "    rgba = np.dstack([r,g,b, alpha]).astype('uint8')\n",
    "    #return Image.fromarray(colored[1], mode='P')\n",
    "    return Image.fromarray(rgba, mode='RGBA')\n",
    "\n",
    "# tms_server = gps.TMS.build(pyramid, display=render_image)\n",
    "# M.add_layer(TMSRasterData(tms_server), name=\"mosaic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n"
     ]
    }
   ],
   "source": [
    "buildings_vector = VectorData(buildings_filename)\n",
    "M.add_layer(buildings_vector, name=buildings_key);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GeoNotebook + GeoPySpark",
   "language": "python",
   "name": "geonotebook3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
