{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Dependencies](https://spacenetchallenge.github.io/#Dependencies)\n",
    "> The [AWS Command Line Interface (CLI)](https://aws.amazon.com/cli/) must be installed with an active AWS account. Configure the AWS CLI using ‘aws configure’"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Accessing the SpaceNet Data on AWS](https://aws.amazon.com/public-datasets/spacenet/#Accessing_the_SpaceNet_Data_on_AWS)\n",
    "> The SpaceNet dataset is being released in several Areas of Interest. All AOIs will follow a similar directory structure and data format. The imagery is GeoTIFF satellite imagery and corresponding GeoJSON building footprints. You can use the following [aws-cli](https://aws.amazon.com/cli/) command to examine all files available in the dataset (details of file structure below):\n",
    "\n",
    "> `aws s3 ls spacenet-dataset --request-payer requester`\n",
    "\n",
    "> For more detailed information on how to access specific files within the dataset, see [here](https://github.com/SpaceNetChallenge/utilities/tree/master/content/download_instructions).\n",
    "\n",
    "> _The spacenet-dataset S3 bucket is provided as a Requester Pays bucket, see [here](https://docs.aws.amazon.com/AmazonS3/latest/dev/RequesterPaysBuckets.html) for more information._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading Rio raster and vector data with [Boto](https://boto3.readthedocs.io/en/latest/index.html)\n",
    "Since the bucket is Request Pays, we cannot successfully curl images. Instead, Boto, the AWS SDK for Python, provides an interface to download files from Request Pays buckets. The [S3Transfer](https://boto3.readthedocs.io/en/latest/reference/customizations/s3.html#boto3.s3.transfer.S3Transfer) class has a download method that can take in a 'RequestPayer' argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "\n",
    "client = boto3.client(\"s3\")\n",
    "transfer = boto3.s3.transfer.S3Transfer(client)\n",
    "\n",
    "bucket = \"spacenet-dataset\"\n",
    "aoi_path = \"AOI_1_Rio\"\n",
    "aoi_data_path = os.path.join(aoi_path, \"srcData\")\n",
    "building_labels_path = os.path.join(aoi_data_path, \"buildingLabels\")\n",
    "mosaic_3band_path = os.path.join(aoi_data_path, \"mosaic_3band\")\n",
    "\n",
    "def download_if_not_exists(key, filename):\n",
    "    if not os.path.exists(filename):\n",
    "        transfer.download_file(\n",
    "            bucket=bucket, key=key, filename=filename,\n",
    "            extra_args={\"RequestPayer\": \"requester\"})\n",
    "\n",
    "def download_rio_vector_file(geojson_name):\n",
    "    filename = os.path.join(\"/tmp\", geojson_name)\n",
    "    key = os.path.join(building_labels_path, geojson_name)\n",
    "    download_if_not_exists(key, filename)\n",
    "    return filename\n",
    "\n",
    "outline_filename = download_rio_vector_file(\"Rio_OUTLINE_Public_AOI.geojson\")\n",
    "buildings_filename = download_rio_vector_file(\"Rio_Buildings_Public_AOI_v2.geojson\")\n",
    "\n",
    "def list_objects(prefix):\n",
    "    return client.list_objects_v2(\n",
    "        Bucket=bucket, Prefix=prefix,\n",
    "        RequestPayer='requester')\n",
    "    \n",
    "def list_keys(prefix):\n",
    "    objects = list_objects(prefix)[\"Contents\"]\n",
    "    return [obj[\"Key\"] for obj in objects]\n",
    "\n",
    "def download_rio_raster_file():\n",
    "    mosaic_3band_key = list_keys(mosaic_3band_path)[0]\n",
    "    mosaic_3band_tiff = mosaic_3band_key.split(\"/\")[-1]\n",
    "    mosaic_3band_filename = os.path.join(\"/tmp\", mosaic_3band_tiff)\n",
    "    download_if_not_exists(mosaic_3band_key, mosaic_3band_filename)\n",
    "    return mosaic_3band_filename\n",
    "\n",
    "mosaic_3band_filename = download_rio_raster_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangling imagery with [GDAL](http://www.gdal.org/gdal_translate.html)\n",
    "Since \"Compression type JPEG is not supported by [this reader](https://github.com/locationtech/geotrellis/blob/master/raster/src/main/scala/geotrellis/raster/io/geotiff/compression/Decompressor.scala#L119-L122)\" at the time of this demo, we need to [gdal_translate](http://www.gdal.org/gdal_translate.html) the image with a different compression type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from osgeo import gdal\n",
    "\n",
    "catalog_uri = os.path.join(\"/tmp\", \"catalog.tif\")\n",
    "\n",
    "if not os.path.exists(catalog_uri):\n",
    "    gdal.Translate(\n",
    "        destName=catalog_uri, srcDS=mosaic_3band_filename,\n",
    "        creationOptions=['COMPRESS=LZW']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingesting imagery for fast viewing with [GeoPySpark](https://github.com/locationtech-labs/geopyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:geopyspark.geotrellis.io.geotiff.GeoTiffRDD.get.\n: java.io.IOException: No matching file(s) for path: file:/home/hadoop/notebooks/catalog.tif\n\tat geotrellis.spark.io.hadoop.HdfsUtils$.listFiles(HdfsUtils.scala:93)\n\tat geotrellis.spark.io.hadoop.package$withHadoopConfigurationMethods.withInputDirectory(package.scala:44)\n\tat geotrellis.spark.io.hadoop.package$withHadoopConfigurationMethods.withInputDirectory(package.scala:61)\n\tat geotrellis.spark.io.hadoop.HadoopGeoTiffRDD$.configuration(HadoopGeoTiffRDD.scala:79)\n\tat geotrellis.spark.io.hadoop.HadoopGeoTiffRDD$.apply(HadoopGeoTiffRDD.scala:91)\n\tat geotrellis.spark.io.hadoop.HadoopGeoTiffRDD$.apply(HadoopGeoTiffRDD.scala:126)\n\tat geotrellis.spark.io.hadoop.HadoopGeoTiffRDD$.multiband(HadoopGeoTiffRDD.scala:207)\n\tat geotrellis.spark.io.hadoop.HadoopGeoTiffRDD$.spatialMultiband(HadoopGeoTiffRDD.scala:256)\n\tat geopyspark.geotrellis.io.geotiff.GeoTiffRDD$.geopyspark$geotrellis$io$geotiff$GeoTiffRDD$$getHadoopGeoTiffRDD(GeoTiffRDD.scala:125)\n\tat geopyspark.geotrellis.io.geotiff.GeoTiffRDD$$anonfun$get$1.apply(GeoTiffRDD.scala:104)\n\tat geopyspark.geotrellis.io.geotiff.GeoTiffRDD$$anonfun$get$1.apply(GeoTiffRDD.scala:93)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat geopyspark.geotrellis.io.geotiff.GeoTiffRDD$.get(GeoTiffRDD.scala:93)\n\tat geopyspark.geotrellis.io.geotiff.GeoTiffRDD.get(GeoTiffRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2e3ac5672952>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mcatalog_uri\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mmax_tile_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     num_partitions=500)\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mlaid_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile_to_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGlobalLayout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_crs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3857\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hadoop/.local/lib/python3.4/site-packages/geopyspark/geotrellis/geotiff.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(layer_type, uri, crs, max_tile_size, num_partitions, chunk_size, time_tag, time_format, s3_client)\u001b[0m\n\u001b[1;32m     77\u001b[0m                                \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                                \u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'uri'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                                inputs)\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mRasterLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrdd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:geopyspark.geotrellis.io.geotiff.GeoTiffRDD.get.\n: java.io.IOException: No matching file(s) for path: file:/home/hadoop/notebooks/catalog.tif\n\tat geotrellis.spark.io.hadoop.HdfsUtils$.listFiles(HdfsUtils.scala:93)\n\tat geotrellis.spark.io.hadoop.package$withHadoopConfigurationMethods.withInputDirectory(package.scala:44)\n\tat geotrellis.spark.io.hadoop.package$withHadoopConfigurationMethods.withInputDirectory(package.scala:61)\n\tat geotrellis.spark.io.hadoop.HadoopGeoTiffRDD$.configuration(HadoopGeoTiffRDD.scala:79)\n\tat geotrellis.spark.io.hadoop.HadoopGeoTiffRDD$.apply(HadoopGeoTiffRDD.scala:91)\n\tat geotrellis.spark.io.hadoop.HadoopGeoTiffRDD$.apply(HadoopGeoTiffRDD.scala:126)\n\tat geotrellis.spark.io.hadoop.HadoopGeoTiffRDD$.multiband(HadoopGeoTiffRDD.scala:207)\n\tat geotrellis.spark.io.hadoop.HadoopGeoTiffRDD$.spatialMultiband(HadoopGeoTiffRDD.scala:256)\n\tat geopyspark.geotrellis.io.geotiff.GeoTiffRDD$.geopyspark$geotrellis$io$geotiff$GeoTiffRDD$$getHadoopGeoTiffRDD(GeoTiffRDD.scala:125)\n\tat geopyspark.geotrellis.io.geotiff.GeoTiffRDD$$anonfun$get$1.apply(GeoTiffRDD.scala:104)\n\tat geopyspark.geotrellis.io.geotiff.GeoTiffRDD$$anonfun$get$1.apply(GeoTiffRDD.scala:93)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat geopyspark.geotrellis.io.geotiff.GeoTiffRDD$.get(GeoTiffRDD.scala:93)\n\tat geopyspark.geotrellis.io.geotiff.GeoTiffRDD.get(GeoTiffRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "import geopyspark as gps\n",
    "from pyspark import SparkContext\n",
    "conf = gps.geopyspark_conf(\"local[*]\", \"spacenet-ingest\")\n",
    "conf.set(key='spark.ui.enabled', value='true')\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "\n",
    "catalog_uri = \"file:///home/hadoop/notebooks/catalog.tif\"\n",
    "# The following operation takes about X seconds on a reasonably capable 4-core laptop\n",
    "rdd = gps.geotrellis.geotiff.get(\n",
    "    gps.geotrellis.constants.LayerType.SPATIAL, \n",
    "    catalog_uri,\n",
    "    max_tile_size=512,\n",
    "    num_partitions=500)\n",
    "\n",
    "laid_out = rdd.tile_to_layout(layout = gps.GlobalLayout(), target_crs=3857)\n",
    "reprojected = laid_out.reproject(\"EPSG:3857\").cache().repartition(600)\n",
    "pyramided = reprojected.pyramid(start_zoom=12, end_zoom=1)\n",
    "\n",
    "for tiled in pyramided:\n",
    "    gps.geotrellis.catalog.write(\"file:///tmp/spacenet-catalog\", \"spacenet-ingest\", tiled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Showing Rio’s outline, imagery, and building footprints on a map with [GeoNotebook](https://github.com/OpenGeoscience/geonotebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from geonotebook.wrappers import VectorData\n",
    "outline_vector = VectorData(outline_filename)\n",
    "outline_polygons = [polygon for polygon in outline_vector.polygons]\n",
    "outline_polygon = outline_polygons[0]\n",
    "outline_centroid = outline_polygon.centroid\n",
    "x = outline_centroid.x\n",
    "y = outline_centroid.y\n",
    "z = 12\n",
    "M.set_center(x, y, z);\n",
    "M.add_layer(outline_vector, name=\"outline\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pyramid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-c0d193902878>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgba\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'RGBA'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mtms_server\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTMS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyramid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrender_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTMSRasterData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtms_server\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mosaic\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pyramid' is not defined"
     ]
    }
   ],
   "source": [
    "def render_image(tile):\n",
    "    cells = tile.cells\n",
    "    # Color correct - use magic numbers\n",
    "    magic_min, magic_max = 4000, 15176\n",
    "    norm_range = magic_max - magic_min\n",
    "    cells = cells.astype('int32')\n",
    "    # Clamp cells\n",
    "    cells[(cells != 0) & (cells < magic_min)] = magic_min\n",
    "    cells[(cells != 0) & (cells > magic_max)] = magic_max\n",
    "    colored = ((cells - magic_min) * 255) / norm_range\n",
    "    (r, g, b) = (colored[2], colored[1], colored[0])\n",
    "    alpha = np.full(r.shape, 255)\n",
    "    alpha[(cells[0] == tile.no_data_value) & \\\n",
    "          (cells[1] == tile.no_data_value) & \\\n",
    "          (cells[2] == tile.no_data_value)] = 0\n",
    "    rgba = np.dstack([r,g,b, alpha]).astype('uint8')\n",
    "    #return Image.fromarray(colored[1], mode='P')\n",
    "    return Image.fromarray(rgba, mode='RGBA')\n",
    "\n",
    "tms_server = gps.TMS.build(pyramid, display=render_image)\n",
    "M.add_layer(TMSRasterData(tms_server), name=\"mosaic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "buildings_vector = VectorData(buildings_filename)\n",
    "# M.add_layer(buildings_vector, name=\"buildings\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GeoNotebook + GeoPySpark",
   "language": "python",
   "name": "geonotebook3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
